{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69aa3385-94a2-4deb-833e-91173df76fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLflow3 Example: Databricks ServicesをMCPで利用するAgent\n",
    "\n",
    "**Databricksにおけるモデルコンテキストプロトコル (MCP)**\n",
    "- https://docs.databricks.com/aws/ja/generative-ai/agent-framework/mcp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e041d869-e698-4789-a410-3de4f80ac46f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installing and Updating Databricks and MLflow Packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qq mlflow[databricks]>=3.1 databricks-agents databricks-langchain langgraph databricks-mcp \"mcp>=1.9\" \"databricks-sdk[openai]\"\n",
    "%pip install nest-asyncio\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2325cf1-105a-418d-a2e2-7344beb5d59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6387f91-a8ed-4654-b1eb-ae72252adbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 開発"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f2ed8a-2b40-4e52-a20e-64ee93a10deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.session import ClientSession\n",
    "from databricks_mcp import DatabricksOAuthClientProvider\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "workspace_hostname = workspace_client.config.host\n",
    "mcp_server_url = f\"{workspace_hostname}/api/2.0/mcp/functions/system/ai\"\n",
    "# mcp_server_url = f\"{workspace_hostname}/api/2.0/mcp/genie/01f046d343c21b2aa054343c77e91ab9\"\n",
    "\n",
    "\n",
    "# This snippet below uses the Unity Catalog functions MCP server to expose built-in\n",
    "# AI tools under `system.ai`, like the `system.ai.python_exec` code interpreter tool\n",
    "async def test_connect_to_server():\n",
    "    async with streamablehttp_client(\n",
    "        f\"{mcp_server_url}\", auth=DatabricksOAuthClientProvider(workspace_client)\n",
    "    ) as (read_stream, write_stream, _), ClientSession(\n",
    "        read_stream, write_stream\n",
    "    ) as session:\n",
    "        # List and call tools from the MCP server\n",
    "        await session.initialize()\n",
    "        tools = await session.list_tools()\n",
    "        print(\n",
    "            f\"Discovered tools {[t.name for t in tools.tools]} \"\n",
    "            f\"from MCP server {mcp_server_url}\"\n",
    "        )\n",
    "        result = await session.call_tool(\n",
    "            \"system__ai__python_exec\", {\"code\": \"print('Hello, world!')\"}\n",
    "        )\n",
    "        print(\n",
    "            f\"Called system__ai__python_exec tool and got result \" f\"{result.content}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(test_connect_to_server())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0754580-f1e2-498e-b167-e4a2d20448e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## カスタムResponseAgentの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d294575-13e8-4864-9cd8-c5575fdd8306",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Simple Response Agent with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agents/simple_responses_agent.py\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from mlflow.entities import SpanType\n",
    "from typing import Generator, Any\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    AIMessageChunk,\n",
    ")\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# ダミーツール\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"指定された都市の天気を取得します\n",
    "\n",
    "    Args:\n",
    "        city (str): 都市名\n",
    "\n",
    "    Returns:\n",
    "        str: 天気情報\n",
    "    \"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "\n",
    "# Tracingの有効化\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Agent\n",
    "class SimpleResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, model, tools: list[BaseTool]):\n",
    "        \"\"\"SimpleResponsesAgentの初期化\n",
    "\n",
    "        Args:\n",
    "            model: 使用するモデル\n",
    "            tools (list[BaseTool]): 使用するツールのリスト\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tools = tools\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"リクエストに基づいて予測を行います\n",
    "\n",
    "        Args:\n",
    "            request (ResponsesAgentRequest): 予測リクエスト\n",
    "\n",
    "        Returns:\n",
    "            ResponsesAgentResponse: 予測結果のレスポンス\n",
    "        \"\"\"\n",
    "        events = [\n",
    "            event\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        outputs = [event.item for event in events]\n",
    "        # usage総量を計算\n",
    "        usages = [event.usage for event in events]\n",
    "        total_usage = {\n",
    "            \"input_tokens_details\": {\"cached_tokens\": 0},\n",
    "            \"output_tokens_details\": {\"reasoning_tokens\": 0},\n",
    "            **reduce(\n",
    "                lambda x, y: {k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y)},\n",
    "                usages,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return ResponsesAgentResponse(output=outputs, usage=total_usage)\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"ストリームモードで予測を行います\n",
    "\n",
    "        Args:\n",
    "            request (ResponsesAgentRequest): 予測リクエスト\n",
    "\n",
    "        Yields:\n",
    "            ResponsesAgentStreamEvent: ストリームイベント\n",
    "        \"\"\"\n",
    "        messages, params = self._convert_request_to_lc_request(request)\n",
    "        react_agent = create_react_agent(self.model.bind(**params), tools=self.tools)\n",
    "\n",
    "        for chunk in react_agent.stream({\"messages\": messages}, stream_mode=\"updates\"):\n",
    "            for value in chunk.values():\n",
    "                messages = value.get(\"messages\", [])\n",
    "                responses = self._convert_lc_messages_to_response(messages)\n",
    "                for response in responses:\n",
    "                    yield response\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.PARSER)\n",
    "    def _convert_request_to_lc_request(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> (list[BaseMessage], dict[str, Any]):\n",
    "        \"\"\"リクエストをLangChainのメッセージおよびパラメータ形式に変換します\n",
    "\n",
    "        Args:\n",
    "            request (ResponsesAgentRequest): 変換するリクエスト\n",
    "\n",
    "        Returns:\n",
    "            tuple: メッセージリスト、パラメータ辞書\n",
    "        \"\"\"\n",
    "\n",
    "        lc_request = request.model_dump_compat(exclude_none=True)\n",
    "        custom_inputs = lc_request.pop(\"custom_inputs\", {})\n",
    "\n",
    "        # custom_inputsは通常のパラメータとして展開\n",
    "        lc_request.update(custom_inputs)\n",
    "        messages = lc_request.pop(\"input\")\n",
    "\n",
    "        # LangChainで有効なパラメータのみに限定\n",
    "        valid_params = [\n",
    "            \"temperature\",\n",
    "            \"max_output_tokens\",\n",
    "            \"top_p\",\n",
    "            \"top_k\",\n",
    "        ]\n",
    "        params = {k: v for k, v in lc_request.items() if k in valid_params}\n",
    "        if \"max_output_tokens\" in params:\n",
    "            params[\"max_tokens\"] = params.pop(\"max_output_tokens\")\n",
    "\n",
    "        return messages, params\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.PARSER)\n",
    "    def _convert_lc_messages_to_response(\n",
    "        self, messages: list[BaseMessage]\n",
    "    ) -> list[ResponsesAgentStreamEvent]:\n",
    "        \"\"\"LangChainメッセージをレスポンス出力に変換します\n",
    "\n",
    "        Args:\n",
    "            messages (list[BaseMessage]): 変換するメッセージリスト\n",
    "\n",
    "        Returns:\n",
    "            list[ResponsesAgentStreamEvent]: レスポンス出力のリスト\n",
    "        \"\"\"\n",
    "\n",
    "        def _create_response_agent_stream_event(\n",
    "            item, usage, metadata\n",
    "        ) -> ResponsesAgentStreamEvent:\n",
    "            return ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=item,\n",
    "                usage=_convert_lc_usage_to_openai_usage(usage),\n",
    "                metadata=metadata,\n",
    "            )\n",
    "\n",
    "        def _convert_lc_usage_to_openai_usage(usage: dict[str, int]) -> dict[str, int]:\n",
    "            return {\n",
    "                \"input_tokens\": usage.get(\"prompt_tokens\", 0),\n",
    "                \"output_tokens\": usage.get(\"completion_tokens\", 0),\n",
    "                \"total_tokens\": usage.get(\"total_tokens\", 0),\n",
    "            }\n",
    "\n",
    "        outputs = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, ToolMessage):\n",
    "                item = self.create_function_call_output_item(\n",
    "                    output=message.content,\n",
    "                    call_id=message.tool_call_id,\n",
    "                )\n",
    "                metadata = message.response_metadata\n",
    "                usage = metadata.pop(\"usage\", {})\n",
    "                outputs.append(\n",
    "                    _create_response_agent_stream_event(item, usage, metadata)\n",
    "                )\n",
    "            elif (\n",
    "                isinstance(message, (AIMessage, AIMessageChunk)) and message.tool_calls\n",
    "            ):\n",
    "                metadata = message.response_metadata\n",
    "                usage = metadata.pop(\"usage\", {})\n",
    "                for tool_call in message.tool_calls:\n",
    "                    item = self.create_function_call_item(\n",
    "                        id=message.id,\n",
    "                        call_id=tool_call.get(\"id\"),\n",
    "                        name=tool_call.get(\"name\"),\n",
    "                        arguments=str(tool_call.get(\"args\")),\n",
    "                    )\n",
    "                    outputs.append(\n",
    "                        _create_response_agent_stream_event(item, usage, metadata)\n",
    "                    )\n",
    "                    # 1件目のみusageを設定\n",
    "                    usage = {}\n",
    "            elif isinstance(message, (AIMessage, AIMessageChunk)):\n",
    "                item = self.create_text_output_item(\n",
    "                    text=message.content,\n",
    "                    id=message.id,\n",
    "                )\n",
    "                metadata = message.response_metadata\n",
    "                usage = metadata.pop(\"usage\", {})\n",
    "                outputs.append(\n",
    "                    _create_response_agent_stream_event(item, usage, metadata)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown message: {message}\")\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Databricksネイティブのllama-3-1-405b-instructを利用\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-405b-instruct\"\n",
    "llm = ChatDatabricks(model=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# 利用可能なツールとして、get_weather関数とUnity Catalogのsystem.ai配下の関数を設定\n",
    "func_name = f\"system.ai.python_exec\"\n",
    "uc_toolkit = UCFunctionToolkit(function_names=[func_name])\n",
    "LC_TOOLS = [get_weather] + uc_toolkit.tools\n",
    "\n",
    "# mlflowにエージェントを設定\n",
    "agent = SimpleResponsesAgent(model=llm, tools=LC_TOOLS)\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f0d395-c876-4e3b-bedc-b30db2d89e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "動作試験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a10963d-d174-4bbf-9061-c9d628be330b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8cc9e0-e298-4619-953d-4d9755c85255",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent Response Predictions for User Queries"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"agents\"))\n",
    "\n",
    "from simple_responses_agent import (\n",
    "    SimpleResponsesAgent,\n",
    "    ResponsesAgentRequest,\n",
    "    agent,\n",
    ")\n",
    "\n",
    "input = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"what is the weather in Tokyo?\"}],\n",
    "    # \"input\": [{\"role\": \"user\", \"content\":\"aa\"}],    \n",
    "    \"context\": {\"conversation_id\": \"123\", \"user_id\": \"456\"},\n",
    "    \"max_output_tokens\": 100,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "print(agent.predict(ResponsesAgentRequest(**input)))\n",
    "\n",
    "input = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"what is 4*3 in python\"}],\n",
    "    \"context\": {\"conversation_id\": \"123\", \"user_id\": \"456\"},\n",
    "    \"top_p\": 0.0,\n",
    "}\n",
    "for event in agent.predict_stream(ResponsesAgentRequest(**input)):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab9c7cc-6a27-4a79-8f6a-2160e7b4433a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## エージェントのロギング・テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a981f80-9820-4b51-abe3-cd9dc552f911",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logging MLflow Model with Databricks Resources"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksFunction,\n",
    ")\n",
    "from simple_responses_agent import LLM_ENDPOINT_NAME\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\"),\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"agents/simple_responses_agent.py\",\n",
    "        name=\"simple_responses_agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.1.0\",\n",
    "            \"langgraph==0.4.8\",\n",
    "            \"databricks-langchain==0.5.1\",\n",
    "            \"unitycatalog-langchain==0.2.0\",\n",
    "            \"unitycatalog-ai==0.3.1\",\n",
    "            \"protobuf==4.25.8\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb01917-5c3e-422d-9dba-04a98272f5aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming Weather Predictions Using MLflow Model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pprint import pprint\n",
    "\n",
    "model_uri = logged_agent_info.model_uri\n",
    "agent = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "input = {\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"what is the weather in Tokyo?\"},\n",
    "    ],\n",
    "    \"max_output_tokens\": 1000,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "for event in agent.predict_stream(input):\n",
    "    pprint(event.get(\"item\"))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "871a7134-dbef-4a74-aa14-fe2f16be1cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## デプロイ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e651086-253d-493d-9818-2cfe79f971b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Registering an ML Model in Databricks UC"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "catalog = \"examples\"\n",
    "schema = \"mlflow\"\n",
    "model_name = f\"{catalog}.{schema}.simple_responses_agent\"\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "registered_model = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02230539-120a-4093-8db6-c3cdc21384a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploying a Model and Retrieving Endpoint URL"
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "deployment = agents.deploy(\n",
    "    registered_model.name, registered_model.version, scale_to_zero=False\n",
    ")\n",
    "\n",
    "# Retrieve the query endpoint URL for making API requests\n",
    "deployment.query_endpoint"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "002_mcp_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
