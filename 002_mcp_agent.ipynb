{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69aa3385-94a2-4deb-833e-91173df76fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLflow3 Example: Databricks Servicesã‚’MCPã§åˆ©ç”¨ã™ã‚‹Agent\n",
    "\n",
    "**Databricksã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ« (MCP)**\n",
    "- https://docs.databricks.com/aws/ja/generative-ai/agent-framework/mcp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e041d869-e698-4789-a410-3de4f80ac46f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installing and Updating Databricks and MLflow Packages"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.11/site-packages (1.5.6)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq mlflow[databricks]>=3.1 databricks-agents databricks-langchain langgraph databricks-mcp \"mcp>=1.9\" \"databricks-sdk[openai]\"\n",
    "%pip install nest-asyncio\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2325cf1-105a-418d-a2e2-7344beb5d59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6387f91-a8ed-4654-b1eb-ae72252adbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## é–‹ç™º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f2ed8a-2b40-4e52-a20e-64ee93a10deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered tools ['system__ai__python_exec'] from MCP server https://dbc-f1d46cb2-e7aa.cloud.databricks.com/api/2.0/mcp/functions/system/ai\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n  |   File \"/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n  |     exec(code_obj, self.user_global_ns, self.user_ns)\n  |   File \"/home/spark-784f8e97-a9d8-46ae-a007-58/.ipykernel/5730/command-8794549490817940-4107433531\", line 38, in <module>\n  |     asyncio.run(test_connect_to_server())\n  |   File \"/databricks/python/lib/python3.11/site-packages/nest_asyncio.py\", line 35, in run\n  |     return loop.run_until_complete(task)\n  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/databricks/python/lib/python3.11/site-packages/nest_asyncio.py\", line 90, in run_until_complete\n  |     return f.result()\n  |            ^^^^^^^^^^\n  |   File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n  |     raise self._exception.with_traceback(self._exception_tb)\n  |   File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n  |     result = coro.send(None)\n  |              ^^^^^^^^^^^^^^^\n  |   File \"/home/spark-784f8e97-a9d8-46ae-a007-58/.ipykernel/5730/command-8794549490817940-4107433531\", line 17, in test_connect_to_server\n  |     async with streamablehttp_client(\n  |   File \"/usr/lib/python3.11/contextlib.py\", line 231, in __aexit__\n  |     await self.gen.athrow(typ, value, traceback)\n  |   File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-784f8e97-a9d8-46ae-a007-58a88d4a5462/lib/python3.11/site-packages/mcp/client/streamable_http.py\", line 437, in streamablehttp_client\n  |     async with anyio.create_task_group() as tg:\n  |   File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-784f8e97-a9d8-46ae-a007-58a88d4a5462/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n    |     result = coro.send(None)\n    |              ^^^^^^^^^^^^^^^\n    |   File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-784f8e97-a9d8-46ae-a007-58a88d4a5462/lib/python3.11/site-packages/mcp/client/streamable_http.py\", line 368, in handle_request_async\n    |     await self._handle_post_request(ctx)\n    |   File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-784f8e97-a9d8-46ae-a007-58a88d4a5462/lib/python3.11/site-packages/mcp/client/streamable_http.py\", line 252, in _handle_post_request\n    |     response.raise_for_status()\n    |   File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-784f8e97-a9d8-46ae-a007-58a88d4a5462/lib/python3.11/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    |     raise HTTPStatusError(message, request=request, response=self)\n    | httpx.HTTPStatusError: Server error '504 Gateway Timeout' for url 'https://dbc-f1d46cb2-e7aa.cloud.databricks.com/api/2.0/mcp/functions/system/ai'\n    | For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/504\n    +------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ExceptionGroup",
        "evalue": "unhandled errors in a TaskGroup (1 sub-exception)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ExceptionGroup</span>: unhandled errors in a TaskGroup (1 sub-exception)"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.session import ClientSession\n",
    "from databricks_mcp import DatabricksOAuthClientProvider\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "workspace_hostname = workspace_client.config.host\n",
    "mcp_server_url = f\"{workspace_hostname}/api/2.0/mcp/functions/system/ai\"\n",
    "# mcp_server_url = f\"{workspace_hostname}/api/2.0/mcp/genie/01f046d343c21b2aa054343c77e91ab9\"\n",
    "\n",
    "\n",
    "# This snippet below uses the Unity Catalog functions MCP server to expose built-in\n",
    "# AI tools under `system.ai`, like the `system.ai.python_exec` code interpreter tool\n",
    "async def test_connect_to_server():\n",
    "    async with streamablehttp_client(\n",
    "        f\"{mcp_server_url}\", auth=DatabricksOAuthClientProvider(workspace_client)\n",
    "    ) as (read_stream, write_stream, _), ClientSession(\n",
    "        read_stream, write_stream\n",
    "    ) as session:\n",
    "        # List and call tools from the MCP server\n",
    "        await session.initialize()\n",
    "        tools = await session.list_tools()\n",
    "        print(\n",
    "            f\"Discovered tools {[t.name for t in tools.tools]} \"\n",
    "            f\"from MCP server {mcp_server_url}\"\n",
    "        )\n",
    "        result = await session.call_tool(\n",
    "            \"system__ai__python_exec\", {\"code\": \"print('Hello, world!')\"}\n",
    "        )\n",
    "        print(\n",
    "            f\"Called system__ai__python_exec tool and got result \" f\"{result.content}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(test_connect_to_server())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0754580-f1e2-498e-b167-e4a2d20448e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ã‚«ã‚¹ã‚¿ãƒ ResponseAgentã®å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d294575-13e8-4864-9cd8-c5575fdd8306",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Simple Response Agent with MLflow"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/simple_responses_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/simple_responses_agent.py\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from mlflow.entities import SpanType\n",
    "from typing import Generator, Any\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    AIMessageChunk,\n",
    ")\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# ãƒ€ãƒŸãƒ¼ãƒ„ãƒ¼ãƒ«\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸéƒ½å¸‚ã®å¤©æ°—ã‚’å–å¾—ã—ã¾ã™\n",
    "\n",
    "    Args:\n",
    "        city (str): éƒ½å¸‚å\n",
    "\n",
    "    Returns:\n",
    "        str: å¤©æ°—æƒ…å ±\n",
    "    \"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "\n",
    "# Tracingã®æœ‰åŠ¹åŒ–\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Agent\n",
    "class SimpleResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, model, tools: list[BaseTool]):\n",
    "        \"\"\"SimpleResponsesAgentã®åˆæœŸåŒ–\n",
    "\n",
    "        Args:\n",
    "            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "            tools (list[BaseTool]): ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tools = tools\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«åŸºã¥ã„ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™\n",
    "\n",
    "        Args:\n",
    "            request (ResponsesAgentRequest): äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n",
    "\n",
    "        Returns:\n",
    "            ResponsesAgentResponse: äºˆæ¸¬çµæžœã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        events = [\n",
    "            event\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        outputs = [event.item for event in events]\n",
    "        # usageç·é‡ã‚’è¨ˆç®—\n",
    "        usages = [event.usage for event in events]\n",
    "        total_usage = {\n",
    "            \"input_tokens_details\": {\"cached_tokens\": 0},\n",
    "            \"output_tokens_details\": {\"reasoning_tokens\": 0},\n",
    "            **reduce(\n",
    "                lambda x, y: {k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y)},\n",
    "                usages,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return ResponsesAgentResponse(output=outputs, usage=total_usage)\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ¢ãƒ¼ãƒ‰ã§äºˆæ¸¬ã‚’è¡Œã„ã¾ã™\n",
    "\n",
    "        Args:\n",
    "            request (ResponsesAgentRequest): äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n",
    "\n",
    "        Yields:\n",
    "            ResponsesAgentStreamEvent: ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ™ãƒ³ãƒˆ\n",
    "        \"\"\"\n",
    "        messages, params = self._convert_request_to_lc_request(request)\n",
    "        react_agent = create_react_agent(self.model.bind(**params), tools=self.tools)\n",
    "\n",
    "        for chunk in react_agent.stream({\"messages\": messages}, stream_mode=\"updates\"):\n",
    "            for value in chunk.values():\n",
    "                messages = value.get(\"messages\", [])\n",
    "                responses = self._convert_lc_messages_to_response(messages)\n",
    "                for response in responses:\n",
    "                    yield response\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.PARSER)\n",
    "    def _convert_request_to_lc_request(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> (list[BaseMessage], dict[str, Any]):\n",
    "        \"\"\"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŠã‚ˆã³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›ã—ã¾ã™\n",
    "\n",
    "        Args:\n",
    "            request (ResponsesAgentRequest): å¤‰æ›ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n",
    "\n",
    "        Returns:\n",
    "            tuple: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾žæ›¸\n",
    "        \"\"\"\n",
    "\n",
    "        lc_request = request.model_dump_compat(exclude_none=True)\n",
    "        custom_inputs = lc_request.pop(\"custom_inputs\", {})\n",
    "\n",
    "        # custom_inputsã¯é€šå¸¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å±•é–‹\n",
    "        lc_request.update(custom_inputs)\n",
    "        messages = lc_request.pop(\"input\")\n",
    "\n",
    "        # LangChainã§æœ‰åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã«é™å®š\n",
    "        valid_params = [\n",
    "            \"temperature\",\n",
    "            \"max_output_tokens\",\n",
    "            \"top_p\",\n",
    "            \"top_k\",\n",
    "        ]\n",
    "        params = {k: v for k, v in lc_request.items() if k in valid_params}\n",
    "        if \"max_output_tokens\" in params:\n",
    "            params[\"max_tokens\"] = params.pop(\"max_output_tokens\")\n",
    "\n",
    "        return messages, params\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.PARSER)\n",
    "    def _convert_lc_messages_to_response(\n",
    "        self, messages: list[BaseMessage]\n",
    "    ) -> list[ResponsesAgentStreamEvent]:\n",
    "        \"\"\"LangChainãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡ºåŠ›ã«å¤‰æ›ã—ã¾ã™\n",
    "\n",
    "        Args:\n",
    "            messages (list[BaseMessage]): å¤‰æ›ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ\n",
    "\n",
    "        Returns:\n",
    "            list[ResponsesAgentStreamEvent]: ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡ºåŠ›ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        def _create_response_agent_stream_event(\n",
    "            item, usage, metadata\n",
    "        ) -> ResponsesAgentStreamEvent:\n",
    "            return ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=item,\n",
    "                usage=_convert_lc_usage_to_openai_usage(usage),\n",
    "                metadata=metadata,\n",
    "            )\n",
    "\n",
    "        def _convert_lc_usage_to_openai_usage(usage: dict[str, int]) -> dict[str, int]:\n",
    "            return {\n",
    "                \"input_tokens\": usage.get(\"prompt_tokens\", 0),\n",
    "                \"output_tokens\": usage.get(\"completion_tokens\", 0),\n",
    "                \"total_tokens\": usage.get(\"total_tokens\", 0),\n",
    "            }\n",
    "\n",
    "        outputs = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, ToolMessage):\n",
    "                item = self.create_function_call_output_item(\n",
    "                    output=message.content,\n",
    "                    call_id=message.tool_call_id,\n",
    "                )\n",
    "                metadata = message.response_metadata\n",
    "                usage = metadata.pop(\"usage\", {})\n",
    "                outputs.append(\n",
    "                    _create_response_agent_stream_event(item, usage, metadata)\n",
    "                )\n",
    "            elif (\n",
    "                isinstance(message, (AIMessage, AIMessageChunk)) and message.tool_calls\n",
    "            ):\n",
    "                metadata = message.response_metadata\n",
    "                usage = metadata.pop(\"usage\", {})\n",
    "                for tool_call in message.tool_calls:\n",
    "                    item = self.create_function_call_item(\n",
    "                        id=message.id,\n",
    "                        call_id=tool_call.get(\"id\"),\n",
    "                        name=tool_call.get(\"name\"),\n",
    "                        arguments=str(tool_call.get(\"args\")),\n",
    "                    )\n",
    "                    outputs.append(\n",
    "                        _create_response_agent_stream_event(item, usage, metadata)\n",
    "                    )\n",
    "                    # 1ä»¶ç›®ã®ã¿usageã‚’è¨­å®š\n",
    "                    usage = {}\n",
    "            elif isinstance(message, (AIMessage, AIMessageChunk)):\n",
    "                item = self.create_text_output_item(\n",
    "                    text=message.content,\n",
    "                    id=message.id,\n",
    "                )\n",
    "                metadata = message.response_metadata\n",
    "                usage = metadata.pop(\"usage\", {})\n",
    "                outputs.append(\n",
    "                    _create_response_agent_stream_event(item, usage, metadata)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown message: {message}\")\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Databricksãƒã‚¤ãƒ†ã‚£ãƒ–ã®llama-3-1-405b-instructã‚’åˆ©ç”¨\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-405b-instruct\"\n",
    "llm = ChatDatabricks(model=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã€get_weatheré–¢æ•°ã¨Unity Catalogã®system.aié…ä¸‹ã®é–¢æ•°ã‚’è¨­å®š\n",
    "func_name = f\"system.ai.python_exec\"\n",
    "uc_toolkit = UCFunctionToolkit(function_names=[func_name])\n",
    "LC_TOOLS = [get_weather] + uc_toolkit.tools\n",
    "\n",
    "# mlflowã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š\n",
    "agent = SimpleResponsesAgent(model=llm, tools=LC_TOOLS)\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f0d395-c876-4e3b-bedc-b30db2d89e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "å‹•ä½œè©¦é¨“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a10963d-d174-4bbf-9061-c9d628be330b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8cc9e0-e298-4619-953d-4d9755c85255",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent Response Predictions for User Queries"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-d380882e-49bc-4108-91cb-39d4d4f55785/lib/python3.11/site-packages/databricks/connect/session.py:454: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type='response.output_item.done' custom_outputs=None item={'type': 'function_call', 'id': 'run--69c56a83-1f65-4567-951b-7cfda9aa5639-0', 'call_id': 'call_804651ef-d4e5-41cd-ae89-07b5c9358c4f', 'name': 'system__ai__python_exec', 'arguments': \"{'code': 'result = 4*3\\\\\\\\nprint(result)'}\", 'content': 'system__ai__python_exec'} usage={'input_tokens': 956, 'output_tokens': 28, 'total_tokens': 984} metadata={'id': 'chatcmpl_72ed6757-9776-4251-adb3-dcbccbd335d2', 'object': 'chat.completion', 'created': 1749889674, 'model': 'meta-llama-3.1-405b-instruct-081924', 'model_name': 'meta-llama-3.1-405b-instruct-081924'}\ntype='response.output_item.done' custom_outputs=None item={'type': 'function_call_output', 'call_id': 'call_804651ef-d4e5-41cd-ae89-07b5c9358c4f', 'output': '{\"format\": \"SCALAR\", \"value\": \"12\\\\n\"}', 'content': '{\"format\": \"SCALAR\", \"value\": \"12\\\\n\"}'} usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0} metadata={}\ntype='response.output_item.done' custom_outputs=None item={'id': 'run--8af27f9c-9fdb-48bc-b215-c6d0039ea1e8-0', 'content': [{'text': 'The answer is 12.', 'type': 'output_text'}], 'role': 'assistant', 'type': 'message'} usage={'input_tokens': 1014, 'output_tokens': 7, 'total_tokens': 1021} metadata={'id': 'chatcmpl_736d910e-de1d-4d9a-b656-554d35c8c025', 'object': 'chat.completion', 'created': 1749889676, 'model': 'meta-llama-3.1-405b-instruct-081924', 'model_name': 'meta-llama-3.1-405b-instruct-081924'}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-1e0257132151447d84b45542b7882451\"",
      "text/plain": [
       "Trace(trace_id=tr-1e0257132151447d84b45542b7882451)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"agents\"))\n",
    "\n",
    "from simple_responses_agent import (\n",
    "    SimpleResponsesAgent,\n",
    "    ResponsesAgentRequest,\n",
    "    agent,\n",
    ")\n",
    "\n",
    "input = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"what is the weather in Tokyo?\"}],\n",
    "    # \"input\": [{\"role\": \"user\", \"content\":\"aa\"}],    \n",
    "    \"context\": {\"conversation_id\": \"123\", \"user_id\": \"456\"},\n",
    "    \"max_output_tokens\": 100,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "print(agent.predict(ResponsesAgentRequest(**input)))\n",
    "\n",
    "input = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"what is 4*3 in python\"}],\n",
    "    \"context\": {\"conversation_id\": \"123\", \"user_id\": \"456\"},\n",
    "    \"top_p\": 0.0,\n",
    "}\n",
    "for event in agent.predict_stream(ResponsesAgentRequest(**input)):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab9c7cc-6a27-4a79-8f6a-2160e7b4433a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a981f80-9820-4b51-abe3-cd9dc552f911",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logging MLflow Model with Databricks Resources"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”— View Logged Model at: https://dbc-f1d46cb2-e7aa.cloud.databricks.com/ml/experiments/e4d5134ffa8248e396d26e1e1d9a67a6/models/m-5a82ded1cce34c1dad1d3d6acf8fcf6a?o=1765512908890676\n2025/06/14 09:19:49 INFO mlflow.pyfunc: Predicting on input example to validate output\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksFunction,\n",
    ")\n",
    "from simple_responses_agent import LLM_ENDPOINT_NAME\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\"),\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"agents/simple_responses_agent.py\",\n",
    "        name=\"simple_responses_agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.1.0\",\n",
    "            \"langgraph==0.4.8\",\n",
    "            \"databricks-langchain==0.5.1\",\n",
    "            \"unitycatalog-langchain==0.2.0\",\n",
    "            \"unitycatalog-ai==0.3.1\",\n",
    "            \"protobuf==4.25.8\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb01917-5c3e-422d-9dba-04a98272f5aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming Weather Predictions Using MLflow Model"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee97e3ae58f2469daf0753fc45adb855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/14 08:46:36 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - protobuf (current: 4.25.8, required: protobuf==3.20.0)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arguments': \"{'city': 'Tokyo'}\",\n 'call_id': 'call_bceea16a-766b-4fce-b3bc-380c03aee5e7',\n 'content': 'get_weather',\n 'id': 'run--48d60513-f67b-4dae-91f7-5cb243c3d438-0',\n 'name': 'get_weather',\n 'type': 'function_call'}\n-----------------\n{'call_id': 'call_bceea16a-766b-4fce-b3bc-380c03aee5e7',\n 'content': \"It's always sunny in Tokyo!\",\n 'output': \"It's always sunny in Tokyo!\",\n 'type': 'function_call_output'}\n-----------------\n{'content': [{'text': \"It's always sunny in Tokyo!\", 'type': 'output_text'}],\n 'id': 'run--e651d622-aa19-465c-934b-51d3523ac68b-0',\n 'role': 'assistant',\n 'type': 'message'}\n-----------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-3a43f4de77cc02ffca366cabc0db7456\"",
      "text/plain": [
       "Trace(trace_id=tr-3a43f4de77cc02ffca366cabc0db7456)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "from pprint import pprint\n",
    "\n",
    "model_uri = logged_agent_info.model_uri\n",
    "agent = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "input = {\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"what is the weather in Tokyo?\"},\n",
    "    ],\n",
    "    \"max_output_tokens\": 1000,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "for event in agent.predict_stream(input):\n",
    "    pprint(event.get(\"item\"))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "871a7134-dbef-4a74-aa14-fe2f16be1cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ãƒ‡ãƒ—ãƒ­ã‚¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e651086-253d-493d-9818-2cfe79f971b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Registering an ML Model in Databricks UC"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'examples.mlflow.simple_responses_agent' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0c986ce63e44239c81d3a7f468d3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e877e1084fd94cef9452b82b14daa8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”— Created version '7' of model 'examples.mlflow.simple_responses_agent': https://dbc-f1d46cb2-e7aa.cloud.databricks.com/explore/data/models/examples/mlflow/simple_responses_agent/version/7?o=1765512908890676\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "catalog = \"examples\"\n",
    "schema = \"mlflow\"\n",
    "model_name = f\"{catalog}.{schema}.simple_responses_agent\"\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "registered_model = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02230539-120a-4093-8db6-c3cdc21384a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploying a Model and Retrieving Endpoint URL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c58a0b726541a0be1a81fde0d56653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8718bc947581479788fe580d8b2190ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Deployment of examples.mlflow.simple_responses_agent version 7 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n\n    View status: https://dbc-f1d46cb2-e7aa.cloud.databricks.com/ml/endpoints/agents_examples-mlflow-simple_responses_agent\n    Review App: https://dbc-f1d46cb2-e7aa.cloud.databricks.com/ml/review-v2/chat?endpoint=agents_examples-mlflow-simple_responses_agent\n    Monitor: https://dbc-f1d46cb2-e7aa.cloud.databricks.com/ml/experiments/e4d5134ffa8248e396d26e1e1d9a67a6?compareRunsMode=TRACES\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://dbc-f1d46cb2-e7aa.cloud.databricks.com/serving-endpoints/agents_examples-mlflow-simple_responses_agent/served-models/examples-mlflow-simple_responses_agent_7/invocations'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import agents\n",
    "\n",
    "deployment = agents.deploy(\n",
    "    registered_model.name, registered_model.version, scale_to_zero=False\n",
    ")\n",
    "\n",
    "# Retrieve the query endpoint URL for making API requests\n",
    "deployment.query_endpoint"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "002_mcp_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
